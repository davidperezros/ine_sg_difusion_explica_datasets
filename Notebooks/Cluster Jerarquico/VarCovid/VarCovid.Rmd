---
title: "Cluster Jerárquico: VarCovid"
subtitle: 'INE-S.G. Difusión'
author: "David Pérez Ros"
date: "diciembre 2023"
output: 
  md_document:
    variant: markdown_github
  rmdformats::readthedown:
      code_folding: show
      cards : TRUE
editor_options:
  markdown:
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = "center", warning = FALSE, message = FALSE)


# fig.align= 'center para que nos centre todas figuras que se muestran
# warning = FALSE para que no muestre mensajes de warning
# message = FALSE para que no muestre mensajes automáticos cuando carga paquetes y demás en el output
```

# Introducción

## dataset

En este cuaderno vamos a analizar el dataset llamado
[*VarCovid*](https://github.com/davidperezros/ine_sg_difusion_explica_datasets/blob/0c24c4e30aaeab265e937150d1470102c61e62ac/Datasets/VarCovid.xlsx).
Este contiene datos relativos a las Tasas de Variación de fallecidos en
el año 2020 (Año Covid) respecto al año anterior. Los datos han sido
extraidos de la **Operación** 30324 Estimación de Defunciones Semanales
(EDeS), que se encuentra dentro de la temática Salud (Sociedad).
Concretamente en este dataset tenemos las siguientes variables:

-   **ccaa**: Comunidades Autónomas
-   **2020SM20**: Tasa de variación del acumulado hasta la semana 20
    incluida del año 2020 respecto al año anterior en ese mismo periodo.
-   **2020SM53**: Tasa de variación del acumulado hasta la semana 53
    incluida del año 2020 respecto al año anterior en ese mismo periodo.
-   **1Ola**: Tasa de variación entre el acumulado entre la semana 11 de
    2020 y la semana 18, ambas incluidas, respecto a las mismas semanas
    del año anterior. Tiempo correspondiente a la primera ola
-   **2Ola**: Tasa de variación entre el acumulado entre la semana 32 de
    2020 y la semana 49, ambas incluidas, respecto a las mismas semanas
    del año anterior. Tiempo correspondiente a la segunda ola.
-   **3Ola**: Tasa de variación entre el acumulado entre la semana 51 de
    2020 y la semana 10 de 2021, ambas incluidas, respecto a las mismas
    semanas del año anterior. Tiempo correspondiente a la tercera ola.

El objetivo de este estudio será aplicar un Análisis **Cluster** para
hacer grupos de comunidades autónomas en función de las variables
**1Ola** y **2Ola**. Concretamente usaremos un cluster jerárquico.

```{r librerias}
# Librerias
library(readxl) # Para leer los excels
```

Cargamos entonces el conjunto de datos:

```{r cargar_datos}
datos <- read_excel("/Users/davpero/ine_sg_difusion_explica_datasets/Datasets/VarCovid.xlsx", sheet = "Datos")
```

```{r}
# Carga de librerías y datos
library(dendextend)

# Lectura de datos

# Preparación de los datos
resultado <- datos[, c("1Ola", "2Ola")]
comunidades <- datos$ccaa

# Dendrograma
W <- hclust(dist(resultado), method = "single")
dendrograma <- as.dendrogram(W)
dendrograma <- set(dendrograma, "labels", comunidades)

plot(dendrograma, horiz = TRUE)
abline(v = 30, col = "red", lty = 2)
title("Dendrograma")

```

```{r}
# Matriz de distancias
distancias <- dist(resultado)
matriz_distancias <- as.matrix(distancias)
df <- as.data.frame(matriz_distancias)
rownames(df) <- comunidades
colnames(df) <- comunidades
print(df)

# Clustering jerárquico
library(stats)
resultado_escalado <- scale(resultado)


clusters <- hclust(dist(resultado), method = "ward.D2")
grupos <- cutree(clusters, k = 3)

# Asignación de clusters a las comunidades
clusters_asignados <- data.frame("Comunidades Autónomas" = comunidades, "Cluster" = grupos)
print(clusters_asignados)

# Representación gráfica
plot(resultado_escalado[,1], resultado_escalado[,2], xlab = "Variación interanual durante la primera ola de COVID", ylab = "Variación interanual durante la segunda ola de COVID", main = "Variación interanual de defunciones")

text(resultado_escalado[,1], resultado_escalado[,2], labels = comunidades, pos = 3, col = "black")

points(resultado_escalado[grupos == 1, 1], resultado_escalado[grupos == 1, 2], col = "purple", pch = 19)
points(resultado_escalado[grupos == 2, 1], resultado_escalado[grupos == 2, 2], col = "blue", pch = 19)
points(resultado_escalado[grupos == 3, 1], resultado_escalado[grupos == 3, 2], col = "green", pch = 19)
legend("bottomright", legend = c("Cluster 1", "Cluster 2", "Cluster 3"), col = c("purple", "blue", "green"), pch = 19)

```

## Descripción del trabajo a realizar

**(Esto irá en la web de explica)** Se pretende hacer un Análisis
Cluster empleando el procedimiento Cluster Jerárquico de las **ccaa** en
función a las variables **1Ola** y **2Ola**.

-   Hacer un análisis exploratorio.
-   Plantear diversos modelos según variables incluidas.
-   Compararlos con ANOVA y ROC CURVE.
-   Para el modelo seleccionado, explicar los coeficientes, odds
    ratio,...

# Análisis Exploratorio (EDA[^1])

[^1]: EDA viene del Inglés *Exploratory Data Analysis* y son los pasos
    relativos en los que se exploran las variables para tener una idea
    de que forma toma el dataset.

Lo primero de todo vamos a cargar las librearias necesarias para
ejecutar el resto del código del trabajo:

```{r librerias2 ,out.width="50%",echo=FALSE}
library(dplyr) # Para tratamiento de dataframes
library(ggplot2) # Nice plots
library(stats) #hclust package
```

# Clustering: Cluster Jerárquico

## Introducción

El **Análisis de clúster** es una técnica de aprendizaje no supervisado
que agrupa datos similares en conjuntos, llamados clústeres. El objetivo
es dividir un conjunto de datos en grupos homogéneos, donde los miembros
de cada grupo son más similares entre sí que con los miembros de otros
grupos, según algún criterio de similitud predefinido.

Concretamente, el **Cluster Jerárquico** realiza estos grupos -o
clusters- de manera jerárquica y ascendente, es decir que sucesivamente
van fusionando grupos desde el elemento individual (mayor nivel de
grupos, uno por individuo) hacia arriba.

La **representación** de la jerarquía de clúster se representa por medio
de un **dendograma**, en el que las sucesivas fusiones de las ramas a
los distintos niveles nos informan de las sucesivas fusiones de los
grupos en grupos de superior nivel (mayor tamaño, menor homogeneidad)
sucesivamente:

Los **pasos** conretos del Cluster Jerárquico son:

1.  **Matriz de distancia o similitud**: Se calcula una matriz que mide
    la distancia o similitud entre cada par de observaciones. Algunas de
    las medidas comunes son:
    -   **Euclidiana**: Mide la distancia más corta entre dos puntos en
        un espacio euclidiano. Es útil cuando las dimensiones tienen una
        **escala similar** y se desea tener en cuenta la magnitud
        absoluta de las diferencias.
    -   **Manhattan (o Cityblock)**: Calcula la suma de las diferencias
        absolutas entre las coordenadas de dos puntos. Es útil cuando
        las dimensiones **no están en la misma escala** y se quiere
        una medida robusta a los valores atípicos.
    -   **Gower**: métrica de distancia utilizada específicamente para
        conjuntos de **datos mixtos** que contienen variables
        numéricas y categóricas. Esta distancia tiene en cuenta
        diferentes tipos de variables al calcular la similitud entre dos
        observaciones. Se define como una combinación ponderada de las
        distancias entre variables.
2.  **Fusión de clústeres**: En el enfoque aglomerativo, se fusionan
    gradualmente los clústeres más cercanos según la medida de distancia
    o similitud elegida. Esto nos lleva a la pregunta, ¿Cómo se calcula
    la distancia entre Clusters calcular la distancia o similitud entre
    clústeres en el proceso de agrupamiento jerárquico?. Existen varios **métodos de enlace**, destacando:
    -   **Enlace Simple (Single Linkage)**: Calcula la distancia entre clústeres como la distancia más corta entre cualquier punto de un clúster y cualquier punto del otro clúster. Es sensible a la presencia de valores atípicos y al fenómeno del encadenamiento.
    -   **Enlace Completo (Complete Linkage)**: Mide la distancia entre clústeres como la distancia más larga entre cualquier punto de un clúster y cualquier punto del otro clúster. Menos sensible a valores atípicos, pero puede generar clústeres de tamaño desigual.
    -   **Enlace Promedio (Average Linkage)**: Calcula la distancia entre clústeres como la media de todas las distancias entre pares de puntos, uno de cada clúster. Más robusto frente a valores atípicos que el enlace simple y menos propenso al encadenamiento que el enlace completo.
    -   **Enlace de Ward**: Minimiza la varianza dentro de los clústeres al fusionarlos. Intenta minimizar la suma de cuadrados dentro de cada clúster después de la fusión.
3.  **Representación jerárquica**: Esto resulta en un dendrograma que
    muestra la jerarquía de agrupamiento, donde la altura en el
    dendrograma indica la distancia o disimilitud en la que se unen los
    clústeres.

El clustering jerárquico permite explorar diferentes niveles de
granularidad en los datos, pero puede ser computacionalmente costoso
para grandes conjuntos de datos. Es **crucial** elegir la medida de
similitud adecuada y el método de enlace (criterio para unir clústeres,
single linkage, complete linkage, average linkage,...) para obtener
resultados significativos.

# Modelo

## Formulación

**IMPORTANTE**: Ver que no hay ningún NA en el dataset.

```{r factor}
ifelse(sum(is.na(data))==0, print("There is no NA in the dataset."), print("There is some NA in the dataset."))
```

Notar que la distancia más apropiada para usar es la Euclidea ya que ambas variables *1Ola* y *2Ola* son del mismo tipo y corresponden a meses consecutivos, es decir, representan el mismo fenómeno demográfico y en la misma escala. Además como estamos interesados en la diferencia de estas variables a la hora de hacer cluster, esta es la distancia más adecuada.

En cuanto al método para hacer los clusters, vamos a dejar el que viene por defecto, el complete. Este se basa en medir la distancia entre clústeres como la distancia más larga entre cualquier punto de un clúster y cualquier punto del otro clúster. Menos sensible a valores atípicos, pero puede generar clústeres de tamaño desigual.

```{r}

# Preparación de los datos
resultado <- datos[, c("1Ola", "2Ola")]
rownames(resultado)<-datos$ccaa # Para que nos salgan luego los nombres
comunidades <- datos$ccaa

# Matriz de distancias
d <- dist(resultado, method = "euclidean")

# Hierarchical clustering using Complete Linkage
hc1 <- hclust(d, method = "complete" )

# Plot the obtained dendrogram
plot(hc1, cex = 0.6, hang = -1)
```

ChatGPT
En el dendrograma mostrado arriba, cada hoja corresponde a una observación. A medida que avanzamos en el árbol, las observaciones similares se combinan en ramas, las cuales a su vez se fusionan a una altura mayor.

La altura de la fusión, representada en el eje vertical, indica la (des) similitud entre dos observaciones. Cuanto mayor sea la altura de la fusión, menos similares son las observaciones. Es importante destacar que las conclusiones sobre la proximidad de dos observaciones solo se pueden inferir en función de la altura donde las ramas que contienen esas dos observaciones se fusionan inicialmente. No podemos usar la proximidad de dos observaciones a lo largo del eje horizontal como criterio de su similitud.

La altura del corte en el dendrograma controla el número de clusters obtenidos. **Cumple el mismo papel que 'k'** en la agrupación **k-means**. Para identificar subgrupos (es decir, clusters), podemos cortar el dendrograma con la función `cutree`. Suponer que queremos 3 clusteres:


```{r}
# Cut tree into 4 groups
sub1 <- cutree(hc1, k = 3)

# Number of members in each cluster
table(sub1)

```


Podemos mostrar los grupos junto al dataframe con la función mutate.
```{r}
datos %>%
  mutate(cluster = sub1) %>%
  head
```
```{r}
hc2 <- hclust(d, method = "ward.D" )
hc3 <- hclust(d, method = "average" )
hc4 <- hclust(d, method = "single" )


```




```{r}
plot(hc1, cex = 0.6, sub="")
rect.hclust(hc1, k = 3, border = 2:5)


plot(hc2, cex = 0.6, sub="")
rect.hclust(hc2, k = 3, border = 2:5)

plot(hc3, cex = 0.6, sub="")
rect.hclust(hc3, k = 3, border = 2:5)

plot(hc4, cex = 0.6, sub="")
rect.hclust(hc4, k = 3, border = 2:5)
```




# Conclusiones

Este modelo de regresión logística parece haber pasado todos los
supuestos de dicha regresión, con una tasa de acierto buena.
